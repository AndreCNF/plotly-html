{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly HTMLs\n",
    "---\n",
    "\n",
    "Notebook where I create plotly plots in HTML, so as to embed in some web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import yaml                                # Save and load YAML files\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Generic data science and machine learning tools\n",
    "import plotly.graph_objects as go          # Plotly for interactive and pretty plots\n",
    "import plotly.io as pio                    # Save Plotly graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medscape burnouts\n",
    "\n",
    "https://www.medscape.com/slideshow/2019-global-burnout-comparison-6011180#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [22, 27, 28, 12, 38, 37]\n",
    "y_data = ['UK', 'US', 'France', 'Germany', 'Portugal', 'Spain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = 'Roboto'\n",
    "font_size = 20\n",
    "font_color = '#ffffff'\n",
    "background_color = '#2f528f'\n",
    "bar_color = '#ffffff'\n",
    "x_suffix = '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    y=y_data,\n",
    "    x=x_data,\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=bar_color\n",
    "    )\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Percentage of burned out physicians',\n",
    "    font=dict(\n",
    "        family=font,\n",
    "        size=font_size,\n",
    "        color=font_color\n",
    "    ),\n",
    "    paper_bgcolor=background_color,\n",
    "    plot_bgcolor=background_color,\n",
    "    xaxis=dict(\n",
    "        ticksuffix=x_suffix\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        categoryorder='category descending'\n",
    "    )\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, file='medscape_burnouts.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thesis model component impact\n",
    "\n",
    "Measuring the average gain in performance that we get from the components of bidirectionality, embedding layer and time awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the metrics\n",
    "metrics_path = 'GitHub/FCUL_ALS_Disease_Progression/metrics/aggregate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_files = os.listdir(metrics_path)\n",
    "try:\n",
    "    metrics_files.remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "metrics_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with all the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "for file_name in metrics_files:\n",
    "    # Load the current metrics file\n",
    "    stream = open(f'{metrics_path}{file_name}', 'r')\n",
    "    model_metrics = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    # Remove the extension from the name\n",
    "    file_name = file_name.split('.yml')[0]\n",
    "    # Define the model name which will appear in the table\n",
    "    model_name = ''\n",
    "    if 'bidir' in file_name:\n",
    "        model_name = 'Bidirectional '\n",
    "    if 'tlstm' in file_name:\n",
    "        model_name += 'TLSTM'\n",
    "    elif 'mf1lstm' in file_name:\n",
    "        model_name += 'MF1-LSTM'\n",
    "    elif 'mf2lstm' in file_name:\n",
    "        model_name += 'MF2-LSTM'\n",
    "    elif 'lstm' in file_name:\n",
    "        model_name += 'LSTM'\n",
    "    elif 'rnn' in file_name:\n",
    "        model_name += 'RNN'\n",
    "    elif 'xgb' in file_name:\n",
    "        model_name += 'XGBoost'\n",
    "    elif 'logreg' in file_name:\n",
    "        model_name += 'Logistic Regression'\n",
    "    elif 'svm' in file_name:\n",
    "        model_name += 'SVM'\n",
    "    if 'embed' in file_name:\n",
    "        model_name += ', embedded'\n",
    "    if 'delta_ts' in file_name:\n",
    "        model_name += ', time aware'\n",
    "    # Create a dictionary entry for the current model\n",
    "    metrics[model_name] = dict()\n",
    "    metrics[model_name]['Avg. Test AUC'] = model_metrics['test']['AUC']['mean']\n",
    "    metrics[model_name]['Std. Test AUC'] = model_metrics['test']['AUC']['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose to have a row per model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.transpose()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by a descending order of performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.sort_values('Avg. Test AUC', ascending=False)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(metrics_df.index)\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_gains = dict()\n",
    "components_str = dict(bidirectionality='Bidirectional ', \n",
    "                      embedding=', embedded', \n",
    "                      time_awareness=', time aware')\n",
    "for component in components_str.keys():\n",
    "    # Find and match the names of the models with and without the component\n",
    "    models_without_comp = [model_name.replace(components_str[component], '') \n",
    "                           for model_name in model_names \n",
    "                           if components_str[component] in model_name]\n",
    "    models_with_comp = [model_name \n",
    "                        for model_name in model_names \n",
    "                        if components_str[component] in model_name]\n",
    "    model_comp_names_match = dict(zip(models_without_comp, models_with_comp))\n",
    "    curr_component_gains = list()\n",
    "    for model_name in models_without_comp:\n",
    "        # Calculate the difference in model performance with and without the component\n",
    "        component_gain = (metrics_df.loc[model_comp_names_match[model_name], 'Avg. Test AUC'] \n",
    "                          - metrics_df.loc[model_name, 'Avg. Test AUC'])\n",
    "        curr_component_gains.append(component_gain)\n",
    "    # Average the component's effect\n",
    "    component_gains[component] = sum(curr_component_gains) / len(curr_component_gains)\n",
    "component_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and match the names of the models with LSTM and with RNN\n",
    "models_with_lstm = [model_name.replace('RNN', 'LSTM')\n",
    "                    for model_name in model_names \n",
    "                    if 'RNN' in model_name]\n",
    "models_with_rnn = [model_name \n",
    "                   for model_name in model_names \n",
    "                   if 'RNN' in model_name]\n",
    "model_comp_names_match = dict(zip(models_with_rnn, models_with_lstm))\n",
    "curr_component_gains = list()\n",
    "for model_name in models_with_rnn:\n",
    "    # Calculate the difference in model performance with LSTM and with RNN\n",
    "    component_gain = (metrics_df.loc[model_comp_names_match[model_name], 'Avg. Test AUC'] \n",
    "                      - metrics_df.loc[model_name, 'Avg. Test AUC'])\n",
    "    curr_component_gains.append(component_gain)\n",
    "# Average LSTM's effect\n",
    "component_gains['LSTM'] = sum(curr_component_gains) / len(curr_component_gains)\n",
    "component_gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df = pd.Series(component_gains, name='Avg. Impact on Test AUC')\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df.index = ['Bidirectionality', 'Embedding', 'Time Awareness', 'LSTM']\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df.index.rename('Component')\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by a descending order of performance gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df = gain_df.sort_values(ascending=False)\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = 'Roboto'\n",
    "font_size = 20\n",
    "font_color = '#ffffff'\n",
    "background_color = '#8f2f2f'\n",
    "marker_color = ['#FF9999',\n",
    "                '#99FFFF',\n",
    "                '#FFFF99',\n",
    "                '#99FF99']\n",
    "marker_color.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_plot_df = gain_df.copy()\n",
    "gain_plot_df = gain_plot_df.sort_values(ascending=True)\n",
    "# Create the figure\n",
    "figure=dict(\n",
    "    data=[dict(\n",
    "        type='bar',\n",
    "        x=gain_plot_df,\n",
    "        y=gain_plot_df.index,\n",
    "        orientation='h',\n",
    "        marker=dict(color=marker_color)\n",
    "    )],\n",
    "    layout=dict(\n",
    "        paper_bgcolor=background_color,\n",
    "        plot_bgcolor=background_color,\n",
    "        title='Average impact on model\\'s test AUC',\n",
    "        yaxis_title=gain_plot_df.index.name,\n",
    "        font=dict(\n",
    "            family=font,\n",
    "            size=font_size,\n",
    "            color=font_color\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(figure)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, file='GitHub/test-plotly-html/thesis_component_impact.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thesis bidir LSTM time aware feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'GitHub/hai-dash/data/ALS/'\n",
    "data_file_name = 'fcul_als_with_shap_for_lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3784valloss_08_07_2020_04_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_path}{data_file_name}.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SHAP values into a NumPy array and the feature names\n",
    "shap_column_names = [feature for feature in df.columns\n",
    "                     if feature.endswith('_shap')]\n",
    "feature_names = [feature.split('_shap')[0] for feature in shap_column_names]\n",
    "shap_values = df[shap_column_names].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = 'Roboto'\n",
    "font_size = 20\n",
    "font_color = 'white'\n",
    "background_color = '#8f8e2f'\n",
    "marker_color = 'white'\n",
    "max_display = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SHAP summary plot\n",
    "fig = du.visualization.shap_summary_plot(shap_values, feature_names,\n",
    "                                         max_display=max_display,\n",
    "                                         background_color=background_color,\n",
    "                                         marker_color=marker_color,\n",
    "                                         output_type='plotly',\n",
    "                                         font_family=font, font_size=font_size,\n",
    "                                         font_color=font_color,\n",
    "                                         xaxis_title='mean(|SHAP value|)')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, file='GitHub/test-plotly-html/thesis_feat_import_bidir_lstm_delta_t.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thesis XGBoost feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Datasets/Thesis/FCUL_ALS/interpreted/'\n",
    "data_file_name = 'fcul_als_with_shap_for_xgb_0.5926valloss_09_07_2020_02_40'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_path}{data_file_name}.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SHAP values into a NumPy array and the feature names\n",
    "shap_column_names = [feature for feature in df.columns\n",
    "                     if feature.endswith('_shap')]\n",
    "feature_names = [feature.split('_shap')[0] for feature in shap_column_names]\n",
    "shap_values = df[shap_column_names].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = 'Roboto'\n",
    "font_size = 20\n",
    "font_color = 'white'\n",
    "background_color = '#8f8e2f'\n",
    "marker_color = 'white'\n",
    "max_display = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SHAP summary plot\n",
    "fig = du.visualization.shap_summary_plot(shap_values, feature_names,\n",
    "                                         max_display=max_display,\n",
    "                                         background_color=background_color,\n",
    "                                         marker_color=marker_color,\n",
    "                                         output_type='plotly',\n",
    "                                         font_family=font, font_size=font_size,\n",
    "                                         font_color=font_color,\n",
    "                                         xaxis_title='mean(|SHAP value|)')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, file='GitHub/test-plotly-html/thesis_feat_import_xgb.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
